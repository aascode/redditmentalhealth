{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This classifies based on Empath, Part of Speech, Polarity and Objectivity.\n",
    "\n",
    "The recall means \"how many of this class you find over the whole number of element of this class\"\n",
    "The precision will be \"how many are correctly classified among that class\"\n",
    "The f1-score is the harmonic mean between precision & recall\n",
    "\n",
    "\n",
    "GaussianNB\n",
    "SGDClassifier- SVM\n",
    "SGDClassifier- LogReg\n",
    "MultinomialNB\n",
    "RandomForestClassifier\n",
    "XGBoost\n",
    "AdaBoost\n",
    "Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Cap/EmpathVariables_PartsOfSpeech.csv', delimiter = ',',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_map = {0: \"NORMAL\", 1: \"DEPRESSED\"}\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]\n",
    "\n",
    "df.Depressed = df.Depressed.apply(lambda x: decode_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Depressed','Body','Username']), df.Depressed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.53      0.52      0.52      7227\n",
      "      NORMAL       0.49      0.49      0.49      6682\n",
      "\n",
      "   micro avg       0.51      0.51      0.51     13909\n",
      "   macro avg       0.51      0.51      0.51     13909\n",
      "weighted avg       0.51      0.51      0.51     13909\n",
      "\n",
      "\n",
      "\n",
      "[[3775 3452]\n",
      " [3406 3276]]\n",
      "\n",
      "\n",
      "0.5069379538428356\n",
      "0.5256928004456204\n",
      "0.5223467552234675\n",
      "0.5240144364242088\n",
      "0.5063095643821618\n",
      "0.012616130717324541\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.82      0.33      0.47      7227\n",
      "      NORMAL       0.56      0.92      0.70      6682\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     13909\n",
      "   macro avg       0.69      0.63      0.58     13909\n",
      "weighted avg       0.70      0.62      0.58     13909\n",
      "\n",
      "\n",
      "\n",
      "[[2389 4838]\n",
      " [ 514 6168]]\n",
      "\n",
      "\n",
      "0.6152131713279172\n",
      "0.8229417843610058\n",
      "0.3305659333056593\n",
      "0.4716683119447187\n",
      "0.6268214281912913\n",
      "0.31182908917398083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.74      0.44      0.55      7227\n",
      "      NORMAL       0.58      0.84      0.68      6682\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     13909\n",
      "   macro avg       0.66      0.64      0.62     13909\n",
      "weighted avg       0.66      0.63      0.61     13909\n",
      "\n",
      "\n",
      "\n",
      "[[3145 4082]\n",
      " [1092 5590]]\n",
      "\n",
      "\n",
      "0.6280106405924222\n",
      "0.7422704743922587\n",
      "0.43517365435173655\n",
      "0.5486741102581997\n",
      "0.635874764919059\n",
      "0.2949943434801865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.76      0.00      0.00      7227\n",
      "      NORMAL       0.48      1.00      0.65      6682\n",
      "\n",
      "   micro avg       0.48      0.48      0.48     13909\n",
      "   macro avg       0.62      0.50      0.33     13909\n",
      "weighted avg       0.63      0.48      0.31     13909\n",
      "\n",
      "\n",
      "\n",
      "[[  13 7214]\n",
      " [   4 6678]]\n",
      "\n",
      "\n",
      "0.48105543173484794\n",
      "0.7647058823529411\n",
      "0.0017988100179881002\n",
      "0.0035891772501380455\n",
      "0.5006000934256358\n",
      "0.017162252194358685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.65      0.70      0.67      7227\n",
      "      NORMAL       0.64      0.58      0.61      6682\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     13909\n",
      "   macro avg       0.64      0.64      0.64     13909\n",
      "weighted avg       0.64      0.64      0.64     13909\n",
      "\n",
      "\n",
      "\n",
      "[[5056 2171]\n",
      " [2774 3908]]\n",
      "\n",
      "\n",
      "0.644474800488892\n",
      "0.6457215836526181\n",
      "0.6995987269959872\n",
      "0.6715813243009896\n",
      "0.6422267804390291\n",
      "0.2865145523245508\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', penalty='l2',alpha=1e-3, max_iter=5, random_state=42).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.74      0.54      0.62      7227\n",
      "      NORMAL       0.61      0.79      0.69      6682\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     13909\n",
      "   macro avg       0.68      0.67      0.66     13909\n",
      "weighted avg       0.68      0.66      0.66     13909\n",
      "\n",
      "\n",
      "\n",
      "[[3881 3346]\n",
      " [1381 5301]]\n",
      "\n",
      "\n",
      "0.6601481055431735\n",
      "0.7375522614975295\n",
      "0.5370139753701397\n",
      "0.6215069260949636\n",
      "0.6651696635306251\n",
      "0.3403175855569219\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.68      0.60      0.64      7227\n",
      "      NORMAL       0.61      0.69      0.65      6682\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     13909\n",
      "   macro avg       0.65      0.65      0.64     13909\n",
      "weighted avg       0.65      0.64      0.64     13909\n",
      "\n",
      "\n",
      "\n",
      "[[4337 2890]\n",
      " [2070 4612]]\n",
      "\n",
      "\n",
      "0.6433963620677259\n",
      "0.6769158732636179\n",
      "0.600110696001107\n",
      "0.6362036086254951\n",
      "0.6451616036126456\n",
      "0.2910034407575635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.65      0.71      0.68      7227\n",
      "      NORMAL       0.65      0.59      0.62      6682\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     13909\n",
      "   macro avg       0.65      0.65      0.65     13909\n",
      "weighted avg       0.65      0.65      0.65     13909\n",
      "\n",
      "\n",
      "\n",
      "[[5112 2115]\n",
      " [2710 3972]]\n",
      "\n",
      "\n",
      "0.6531023078582213\n",
      "0.6535412937867553\n",
      "0.7073474470734745\n",
      "0.6793806897468271\n",
      "0.6508901258115053\n",
      "0.3039222688311758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = BaggingClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
