{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Groups by Empath variables by Username by adding the values, to put into models.\n",
    "\n",
    "The recall means \"how many of this class you find over the whole number of element of this class\"\n",
    "The precision will be \"how many are correctly classified among that class\"\n",
    "The f1-score is the harmonic mean between precision & recall\n",
    "\n",
    "\n",
    "GaussianNB\n",
    "SGDClassifier- SVM\n",
    "SGDClassifier- LogReg\n",
    "MultinomialNB\n",
    "RandomForestClassifier\n",
    "XGBoost\n",
    "AdaBoost\n",
    "Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Cap/Empath_variables.csv', delimiter = ',',encoding='utf-8')\n",
    "df=df.drop(['Body','Time','Subreddit'], axis = 1)\n",
    "df['count']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf= df.groupby(['Username','Depressed']).mean()\n",
    "ddf=ddf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_map = {0: \"NORMAL\", 1: \"DEPRESSED\"}\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]\n",
    "\n",
    "ddf.Depressed = ddf.Depressed.apply(lambda x: decode_sentiment(x))\n",
    "ddf=df.drop(['Username'], axis = 1)\n",
    "df1=ddf.drop(['Depressed'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df1, ddf.Depressed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.48      0.48      6651\n",
      "           1       0.53      0.53      0.53      7258\n",
      "\n",
      "   micro avg       0.50      0.50      0.50     13909\n",
      "   macro avg       0.50      0.50      0.50     13909\n",
      "weighted avg       0.50      0.50      0.50     13909\n",
      "\n",
      "\n",
      "\n",
      "[[3212 3439]\n",
      " [3447 3811]]\n",
      "\n",
      "\n",
      "0.5049248687899921\n",
      "0.5256551724137931\n",
      "0.525075778451364\n",
      "0.5253653156878965\n",
      "0.5040053377296664\n",
      "0.008010277883747547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.92      0.69      6651\n",
      "           1       0.82      0.32      0.46      7258\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     13909\n",
      "   macro avg       0.68      0.62      0.58     13909\n",
      "weighted avg       0.69      0.61      0.57     13909\n",
      "\n",
      "\n",
      "\n",
      "[[6121  530]\n",
      " [4922 2336]]\n",
      "\n",
      "\n",
      "0.6080235818534762\n",
      "0.8150732728541521\n",
      "0.3218517497933315\n",
      "0.46147767680758583\n",
      "0.621082242360205\n",
      "0.29907545876771086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.83      0.68      6651\n",
      "           1       0.73      0.43      0.54      7258\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     13909\n",
      "   macro avg       0.65      0.63      0.61     13909\n",
      "weighted avg       0.65      0.62      0.61     13909\n",
      "\n",
      "\n",
      "\n",
      "[[5515 1136]\n",
      " [4151 3107]]\n",
      "\n",
      "\n",
      "0.6198864044863038\n",
      "0.7322649069054914\n",
      "0.4280793607054285\n",
      "0.5403008434049213\n",
      "0.6286389887273948\n",
      "0.2791223786834199\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.92      0.70      6651\n",
      "           1       0.82      0.35      0.49      7258\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     13909\n",
      "   macro avg       0.69      0.63      0.59     13909\n",
      "weighted avg       0.70      0.62      0.59     13909\n",
      "\n",
      "\n",
      "\n",
      "[[6103  548]\n",
      " [4745 2513]]\n",
      "\n",
      "\n",
      "0.6194550291178373\n",
      "0.8209735380594577\n",
      "0.3462386332322954\n",
      "0.48706269987401885\n",
      "0.6319225041067507\n",
      "0.3181224619459508\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.85      0.69      6651\n",
      "           1       0.76      0.45      0.56      7258\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     13909\n",
      "   macro avg       0.67      0.65      0.63     13909\n",
      "weighted avg       0.68      0.64      0.62     13909\n",
      "\n",
      "\n",
      "\n",
      "[[5621 1030]\n",
      " [4005 3253]]\n",
      "\n",
      "\n",
      "0.6380041699618951\n",
      "0.7595143590940929\n",
      "0.4481950950675117\n",
      "0.5637293128844987\n",
      "0.6466655824157286\n",
      "0.3174045856232785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', penalty='l2',alpha=1e-3, max_iter=5, random_state=42).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.80      0.68      6651\n",
      "           1       0.72      0.49      0.58      7258\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     13909\n",
      "   macro avg       0.66      0.64      0.63     13909\n",
      "weighted avg       0.66      0.64      0.63     13909\n",
      "\n",
      "\n",
      "\n",
      "[[5307 1344]\n",
      " [3717 3541]]\n",
      "\n",
      "\n",
      "0.6361348766985405\n",
      "0.7248720573183214\n",
      "0.48787544778175806\n",
      "0.5832166680391996\n",
      "0.6429002859116278\n",
      "0.2990771420244862\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.79      0.67      6651\n",
      "           1       0.72      0.49      0.58      7258\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     13909\n",
      "   macro avg       0.65      0.64      0.63     13909\n",
      "weighted avg       0.65      0.63      0.62     13909\n",
      "\n",
      "\n",
      "\n",
      "[[5247 1404]\n",
      " [3724 3534]]\n",
      "\n",
      "\n",
      "0.631317851750665\n",
      "0.715674362089915\n",
      "0.4869109947643979\n",
      "0.5795342735323057\n",
      "0.6379074594931597\n",
      "0.2879212627963906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      6651\n",
      "           1       0.70      0.53      0.61      7258\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     13909\n",
      "   macro avg       0.65      0.64      0.64     13909\n",
      "weighted avg       0.65      0.64      0.63     13909\n",
      "\n",
      "\n",
      "\n",
      "[[5010 1641]\n",
      " [3392 3866]]\n",
      "\n",
      "\n",
      "0.638147961751384\n",
      "0.7020156164881061\n",
      "0.5326536235877652\n",
      "0.6057187622405014\n",
      "0.6429619042611808\n",
      "0.2920474368655534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = BaggingClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
