{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Groups by POS by Username by adding the values, to put into models.\n",
    "\n",
    "The recall means \"how many of this class you find over the whole number of element of this class\"\n",
    "The precision will be \"how many are correctly classified among that class\"\n",
    "The f1-score is the harmonic mean between precision & recall\n",
    "\n",
    "\n",
    "GaussianNB\n",
    "SGDClassifier- SVM\n",
    "SGDClassifier- LogReg\n",
    "MultinomialNB\n",
    "RandomForestClassifier\n",
    "XGBoost\n",
    "AdaBoost\n",
    "Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Cap/PartsOfSpeech.csv', delimiter = ',',encoding='utf-8')\n",
    "df=df.drop(['Body'], axis = 1)\n",
    "df['count']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf= df.groupby(['Username','Depressed']).mean()\n",
    "ddf=ddf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_map = {0: \"NORMAL\", 1: \"DEPRESSED\"}\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]\n",
    "\n",
    "ddf.Depressed = ddf.Depressed.apply(lambda x: decode_sentiment(x))\n",
    "ddf=df.drop(['Username'], axis = 1)\n",
    "df1=ddf.drop(['Depressed'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df1, ddf.Depressed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.47      0.47      6698\n",
      "           1       0.51      0.51      0.51      7211\n",
      "\n",
      "   micro avg       0.49      0.49      0.49     13909\n",
      "   macro avg       0.49      0.49      0.49     13909\n",
      "weighted avg       0.49      0.49      0.49     13909\n",
      "\n",
      "\n",
      "\n",
      "[[3165 3533]\n",
      " [3504 3707]]\n",
      "\n",
      "\n",
      "0.49406858868358616\n",
      "0.5120165745856353\n",
      "0.5140757176535848\n",
      "0.513044079994464\n",
      "0.4933024154108474\n",
      "-0.013397349300659148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.94      0.65      6698\n",
      "           1       0.70      0.14      0.23      7211\n",
      "\n",
      "   micro avg       0.52      0.52      0.52     13909\n",
      "   macro avg       0.60      0.54      0.44     13909\n",
      "weighted avg       0.60      0.52      0.43     13909\n",
      "\n",
      "\n",
      "\n",
      "[[6265  433]\n",
      " [6212  999]]\n",
      "\n",
      "\n",
      "0.522251779423395\n",
      "0.6976256983240223\n",
      "0.13853834419636665\n",
      "0.23116973273169036\n",
      "0.5369460905813126\n",
      "0.12149046916104468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.67      0.61      6698\n",
      "           1       0.63      0.53      0.58      7211\n",
      "\n",
      "   micro avg       0.60      0.60      0.60     13909\n",
      "   macro avg       0.60      0.60      0.60     13909\n",
      "weighted avg       0.60      0.60      0.60     13909\n",
      "\n",
      "\n",
      "\n",
      "[[4460 2238]\n",
      " [3356 3855]]\n",
      "\n",
      "\n",
      "0.5978143647997699\n",
      "0.6326932545544067\n",
      "0.5345999167937873\n",
      "0.5795249549007817\n",
      "0.6002351629355618\n",
      "0.20188895091736586\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.71      0.63      6698\n",
      "           1       0.65      0.51      0.57      7211\n",
      "\n",
      "   micro avg       0.60      0.60      0.60     13909\n",
      "   macro avg       0.61      0.61      0.60     13909\n",
      "weighted avg       0.61      0.60      0.60     13909\n",
      "\n",
      "\n",
      "\n",
      "[[4746 1952]\n",
      " [3563 3648]]\n",
      "\n",
      "\n",
      "0.6034941404845783\n",
      "0.6514285714285715\n",
      "0.5058937734017473\n",
      "0.5695105768480213\n",
      "0.6072317478534566\n",
      "0.21850194490040747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.61      0.60      6698\n",
      "           1       0.63      0.61      0.62      7211\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     13909\n",
      "   macro avg       0.61      0.61      0.61     13909\n",
      "weighted avg       0.61      0.61      0.61     13909\n",
      "\n",
      "\n",
      "\n",
      "[[4056 2642]\n",
      " [2790 4421]]\n",
      "\n",
      "\n",
      "0.6094614997483644\n",
      "0.6259379866912077\n",
      "0.61309111080294\n",
      "0.6194479473167998\n",
      "0.6093225037442589\n",
      "0.21852283894482263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', penalty='l2',alpha=1e-3, max_iter=5, random_state=42).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.67      0.63      6698\n",
      "           1       0.65      0.57      0.61      7211\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     13909\n",
      "   macro avg       0.62      0.62      0.62     13909\n",
      "weighted avg       0.62      0.62      0.62     13909\n",
      "\n",
      "\n",
      "\n",
      "[[4508 2190]\n",
      " [3092 4119]]\n",
      "\n",
      "\n",
      "0.6202458839600259\n",
      "0.6528768426058013\n",
      "0.5712106503952296\n",
      "0.609319526627219\n",
      "0.6221236888882686\n",
      "0.24513942856643797\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.65      0.62      6698\n",
      "           1       0.64      0.58      0.61      7211\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     13909\n",
      "   macro avg       0.61      0.61      0.61     13909\n",
      "weighted avg       0.61      0.61      0.61     13909\n",
      "\n",
      "\n",
      "\n",
      "[[4346 2352]\n",
      " [3058 4153]]\n",
      "\n",
      "\n",
      "0.6110432094327414\n",
      "0.6384319754035357\n",
      "0.5759256691166273\n",
      "0.6055701370662\n",
      "0.6123880361110159\n",
      "0.22509380464978335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.64      0.61      6698\n",
      "           1       0.63      0.57      0.60      7211\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     13909\n",
      "   macro avg       0.61      0.61      0.61     13909\n",
      "weighted avg       0.61      0.61      0.61     13909\n",
      "\n",
      "\n",
      "\n",
      "[[4281 2417]\n",
      " [3067 4144]]\n",
      "\n",
      "\n",
      "0.6057229132216551\n",
      "0.631611034903216\n",
      "0.5746775759256691\n",
      "0.6018007551553878\n",
      "0.6069117948305564\n",
      "0.21402097729874667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = BaggingClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
