{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This classifies based on Part of Speech, Polarity and Objectivity.\n",
    "\n",
    "The recall means \"how many of this class you find over the whole number of element of this class\"\n",
    "The precision will be \"how many are correctly classified among that class\"\n",
    "The f1-score is the harmonic mean between precision & recall\n",
    "\n",
    "\n",
    "GaussianNB\n",
    "SGDClassifier- SVM\n",
    "SGDClassifier- LogReg\n",
    "MultinomialNB\n",
    "RandomForestClassifier\n",
    "XGBoost\n",
    "AdaBoost\n",
    "Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Cap/PartsOfSpeech.csv', delimiter = ',',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_map = {0: \"NORMAL\", 1: \"DEPRESSED\"}\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]\n",
    "\n",
    "df.Depressed = df.Depressed.apply(lambda x: decode_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Depressed','Body','Username']), df.Depressed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.52      0.53      0.52      7203\n",
      "      NORMAL       0.49      0.48      0.48      6706\n",
      "\n",
      "   micro avg       0.51      0.51      0.51     13909\n",
      "   macro avg       0.50      0.50      0.50     13909\n",
      "weighted avg       0.51      0.51      0.51     13909\n",
      "\n",
      "\n",
      "\n",
      "[[3788 3415]\n",
      " [3468 3238]]\n",
      "\n",
      "\n",
      "0.5051405564742253\n",
      "0.5220507166482911\n",
      "0.5258919894488407\n",
      "0.5239643128847085\n",
      "0.5043715837491743\n",
      "0.008745806848547855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.61      0.46      0.52      7203\n",
      "      NORMAL       0.54      0.68      0.60      6706\n",
      "\n",
      "   micro avg       0.57      0.57      0.57     13909\n",
      "   macro avg       0.57      0.57      0.56     13909\n",
      "weighted avg       0.57      0.57      0.56     13909\n",
      "\n",
      "\n",
      "\n",
      "[[3300 3903]\n",
      " [2146 4560]]\n",
      "\n",
      "\n",
      "0.5651017326910633\n",
      "0.6059493206022769\n",
      "0.45814244064972925\n",
      "0.5217803778954858\n",
      "0.5690652555172296\n",
      "0.14140901968624717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.63      0.52      0.57      7203\n",
      "      NORMAL       0.56      0.67      0.61      6706\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     13909\n",
      "   macro avg       0.60      0.59      0.59     13909\n",
      "weighted avg       0.60      0.59      0.59     13909\n",
      "\n",
      "\n",
      "\n",
      "[[3748 3455]\n",
      " [2225 4481]]\n",
      "\n",
      "\n",
      "0.5916313178517507\n",
      "0.627490373346727\n",
      "0.5203387477439956\n",
      "0.5689131754705525\n",
      "0.594273161524846\n",
      "0.19033097063386367\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.60      0.70      0.64      7203\n",
      "      NORMAL       0.60      0.50      0.54      6706\n",
      "\n",
      "   micro avg       0.60      0.60      0.60     13909\n",
      "   macro avg       0.60      0.60      0.59     13909\n",
      "weighted avg       0.60      0.60      0.60     13909\n",
      "\n",
      "\n",
      "\n",
      "[[5019 2184]\n",
      " [3378 3328]]\n",
      "\n",
      "\n",
      "0.600115033431591\n",
      "0.5977134690961058\n",
      "0.6967930029154519\n",
      "0.6434615384615385\n",
      "0.5965324990718028\n",
      "0.19723107692960454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.61      0.57      0.59      7203\n",
      "      NORMAL       0.57      0.61      0.59      6706\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     13909\n",
      "   macro avg       0.59      0.59      0.59     13909\n",
      "weighted avg       0.59      0.59      0.59     13909\n",
      "\n",
      "\n",
      "\n",
      "[[4096 3107]\n",
      " [2626 4080]]\n",
      "\n",
      "\n",
      "0.587820835430297\n",
      "0.6093424576019042\n",
      "0.5686519505761488\n",
      "0.5882944344703769\n",
      "0.5885311646707169\n",
      "0.1770482256635464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', penalty='l2',alpha=1e-3, max_iter=5, random_state=42).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.65      0.56      0.60      7203\n",
      "      NORMAL       0.59      0.67      0.63      6706\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     13909\n",
      "   macro avg       0.62      0.62      0.61     13909\n",
      "weighted avg       0.62      0.61      0.61     13909\n",
      "\n",
      "\n",
      "\n",
      "[[4056 3147]\n",
      " [2223 4483]]\n",
      "\n",
      "\n",
      "0.6139190452225178\n",
      "0.6459627329192547\n",
      "0.5630987088713036\n",
      "0.6016911437472184\n",
      "0.6158022622793738\n",
      "0.23255624735145747\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.64      0.56      0.59      7203\n",
      "      NORMAL       0.58      0.66      0.62      6706\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     13909\n",
      "   macro avg       0.61      0.61      0.61     13909\n",
      "weighted avg       0.61      0.61      0.61     13909\n",
      "\n",
      "\n",
      "\n",
      "[[4010 3193]\n",
      " [2289 4417]]\n",
      "\n",
      "\n",
      "0.6058667050111438\n",
      "0.6366089855532624\n",
      "0.5567124809107317\n",
      "0.5939860761368686\n",
      "0.6076881820002511\n",
      "0.21620134444043673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DEPRESSED       0.62      0.69      0.65      7203\n",
      "      NORMAL       0.62      0.54      0.58      6706\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     13909\n",
      "   macro avg       0.62      0.61      0.61     13909\n",
      "weighted avg       0.62      0.62      0.61     13909\n",
      "\n",
      "\n",
      "\n",
      "[[4937 2266]\n",
      " [3082 3624]]\n",
      "\n",
      "\n",
      "0.6155007549068948\n",
      "0.6156628008479861\n",
      "0.6854088574205193\n",
      "0.6486664038891079\n",
      "0.6129102145736656\n",
      "0.22836732048978453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = BaggingClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred)) \n",
    "print(\"\\n\")\n",
    "\n",
    "y_test1= [0 if x==\"NORMAL\" else x for x in y_test]\n",
    "y_test1= [1 if x==\"DEPRESSED\" else x for x in y_test1]\n",
    "\n",
    "y_pred= [0 if x==\"NORMAL\" else x for x in y_pred]\n",
    "y_pred= [1 if x==\"DEPRESSED\" else x for x in y_pred]\n",
    "\n",
    "print(metrics.accuracy_score(y_test1, y_pred))            #Accuracy\n",
    "print(precision_score(y_test1, y_pred))                   #Precision\n",
    "print(recall_score(y_test1, y_pred))                      #Recall\n",
    "print(f1_score(y_test1, y_pred))                          #F1 Score\n",
    "print(roc_auc_score(y_test1, y_pred))                     #AUC\n",
    "print(matthews_corrcoef(y_test1, y_pred))                 #MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
